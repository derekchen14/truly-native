{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sponsored Posts = SP = 1\n",
    "# Organic Content = OC = 0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn import decomposition, pipeline, metrics, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all the Data\n",
    "training_raw = []\n",
    "testing_raw =[]\n",
    "\n",
    "For folder in np.arange(5)\n",
    "  load data from folder\n",
    "  if part-of-training\n",
    "    put into training_raw\n",
    "  else\n",
    "    put into testing_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of words\n",
    "corpus = []\n",
    "for htmlfile in training_raw\n",
    "    read the htmlfile\n",
    "    words = split the htmlfile by spaces\n",
    "    training_data_words = words\n",
    "    for word in training_data_words\n",
    "        add_to_Corpus(word)\n",
    "\n",
    "# Remove words that probably don't help with training\n",
    "for word in corpus\n",
    "    corpus.remove(word) if count <= 1  \n",
    "    corpus.remove(word) if word == \"a\"\n",
    "    corpus.remove(word) if word == \"the\" \n",
    "    corpus.remove(word) if word == \"an\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate each file of words into a vector\n",
    "training_processed = np.zeroes(corpus.size)\n",
    "testing_processed = np.zeroes(corpus.size)\n",
    "\n",
    "for word in training_data_words\n",
    "    location = corpus.indexOf(word)\n",
    "    training_processed[location]++ if location > 0    \n",
    "    \n",
    "for word in testing_data_words\n",
    "    location = corpus.indexOf(word)\n",
    "    training_processed[location]++ if location > 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_Corpus(word):\n",
    "    if the word is in the corpus\n",
    "        addCount(word)\n",
    "    else\n",
    "        instance = createCount(word)\n",
    "        corpus.append(word)\n",
    "        \n",
    "def addCount:\n",
    "    word[1].count ++\n",
    "    \n",
    "def createCount(word):\n",
    "    create tuple [word, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pull out the correct answer\n",
    "Load train.csv\n",
    "Parse it to only include the words in the corpus\n",
    "return training_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use SKlearn, rather than doing all the math ourselves?\n",
    "\n",
    "I think our original idea was to choose ~10 words that we felt might indicate SP over OC, which \n",
    "would means we have 10 features to feed into the algorithm.  This way, we use all the words in our\n",
    "corpus.  Am I missing something here?\n",
    "'''\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(training_processed, training_answers)\n",
    "testing_predictions = .predict(testing_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create submission\n",
    "final_submission = []\n",
    "For htmlfile in testing_raw\n",
    "    prediction = testing_predictions(htmlfile.name)\n",
    "    final_submission.append([htmlfile.name, prediction])\n",
    "turn final_submission into CSV\n",
    "return final_CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
